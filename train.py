import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import os
import numpy as np

# ä»Žæˆ‘ä»¬è‡ªå·±åˆ›å»ºçš„æ¨¡å—ä¸­å¯¼å…¥
from data_loader import MultiInputSignalDataset
from model import MultiInputModel

# --- 1. å®šä¹‰æŸå¤±å‡½æ•° ---
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, logits, targets):
        # logitså·²ç»æ˜¯sigmoidä¹‹åŽçš„å€¼
        probs = logits 
        
        intersection = (probs * targets).sum()
        dice_coeff = (2. * intersection + self.smooth) / (probs.sum() + targets.sum() + self.smooth)
        
        return 1 - dice_coeff

class CombinedLoss(nn.Module):
    def __init__(self, bce_weight=0.5, dice_weight=0.5):
        super(CombinedLoss, self).__init__()
        # ä½¿ç”¨BCEWithLogitsLossæ›´ç¨³å®šï¼Œå®ƒå†…éƒ¨åŒ…å«äº†sigmoid
        self.bce_loss = nn.BCEWithLogitsLoss()
        self.dice_loss = DiceLoss()
        self.bce_weight = bce_weight
        self.dice_weight = dice_weight

    def forward(self, logits, targets):
        # æ³¨æ„ï¼šBCEWithLogitsLosséœ€è¦æœªç»è¿‡sigmoidçš„åŽŸå§‹è¾“å‡º
        # è€Œæˆ‘ä»¬çš„æ¨¡åž‹æœ€åŽåŠ äº†Sigmoidï¼Œä¸ºä½¿ç”¨æ­¤æŸå¤±ï¼Œæš‚æ—¶å…ˆé€†æ“ä½œ
        # æ›´å¥½çš„åšæ³•æ˜¯ç§»é™¤æ¨¡åž‹æœ€åŽçš„Sigmoidï¼Œåœ¨è¿™é‡Œç»Ÿä¸€å¤„ç†
        # å‡è®¾modelçš„è¾“å‡ºä¸ºlogits (w/o sigmoid)
        bce = self.bce_loss(logits, targets)
        
        # DiceLosséœ€è¦ç»è¿‡sigmoidçš„æ¦‚çŽ‡å€¼
        probs = torch.sigmoid(logits)
        dice = self.dice_loss(probs, targets)
        
        return self.bce_weight * bce + self.dice_weight * dice

# --- 2. è¯„ä¼°æŒ‡æ ‡ ---
def dice_coefficient(probs, targets, smooth=1e-6):
    """è®¡ç®—Diceç³»æ•°ç”¨äºŽè¯„ä¼°"""
    intersection = (probs * targets).sum()
    return (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)

# --- 3. è®­ç»ƒå’ŒéªŒè¯å‡½æ•° ---
def train_one_epoch(model, loader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    
    progress_bar = tqdm(loader, desc="Training")
    for inputs, targets in progress_bar:
        # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        for key in inputs:
            inputs[key] = inputs[key].to(device)
        targets = targets.to(device)
        
        # æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        # æ³¨æ„ï¼šä¸ºä½¿ç”¨BCEWithLogitsLossï¼Œç†æƒ³æƒ…å†µä¸‹æ¨¡åž‹ä¸åº”æœ‰æœ€åŽçš„sigmoidæ¿€æ´»
        # æˆ‘ä»¬åœ¨è¿™é‡Œå‡è®¾æ¨¡åž‹è¾“å‡ºçš„æ˜¯logits
        logits = model(inputs)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(logits, targets)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ›´æ–°æƒé‡
        optimizer.step()
        
        running_loss += loss.item()
        progress_bar.set_postfix(loss=f"{loss.item():.4f}")

    return running_loss / len(loader)

def validate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    total_dice = 0.0
    
    with torch.no_grad():
        progress_bar = tqdm(loader, desc="Validating")
        for inputs, targets in progress_bar:
            for key in inputs:
                inputs[key] = inputs[key].to(device)
            targets = targets.to(device)
            
            logits = model(inputs)
            loss = criterion(logits, targets)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            probs = torch.sigmoid(logits)
            dice = dice_coefficient(probs, targets)
            
            running_loss += loss.item()
            total_dice += dice.item()
            progress_bar.set_postfix(val_loss=f"{loss.item():.4f}", dice=f"{dice.item():.4f}")

    avg_loss = running_loss / len(loader)
    avg_dice = total_dice / len(loader)
    return avg_loss, avg_dice

# --- 4. ä¸»è®­ç»ƒå‡½æ•° ---
def train(config: dict):
    """
    ä¸»è®­ç»ƒæµç¨‹å‡½æ•°
    """
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # åˆ›å»ºæ•°æ®é›†
    full_dataset = MultiInputSignalDataset(h5_path=config['data_path'], config=config)
    
    # # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    # val_percent = config.get('validation_split', 0.2)
    # n_val = int(len(full_dataset) * val_percent)
    # n_train = len(full_dataset) - n_val
    # train_dataset, val_dataset = random_split(full_dataset, [n_train, n_val])

    # --- æ·»åŠ ä»¥ä¸‹ä»£ç æ¥åˆ›å»ºä¸€ä¸ªç”¨äºŽå¿«é€Ÿè°ƒè¯•çš„å­é›† ---
    from torch.utils.data import Subset
    # åªå–å‰400ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼Œæ‚¨å¯ä»¥æŒ‰éœ€è°ƒæ•´æ•°é‡
    subset_indices = range(400) 
    debug_dataset = Subset(full_dataset, indices=subset_indices)
    print(f"--- è°ƒè¯•æ¨¡å¼: ä»…ä½¿ç”¨ {len(debug_dataset)} ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€ŸéªŒè¯ ---")
    # --------------------------------------------------

    # ä½¿ç”¨ debug_dataset æ›¿ä»£ full_dataset è¿›è¡ŒåŽç»­æ“ä½œ
    val_percent = config.get('validation_split', 0.2)
    n_val = int(len(debug_dataset) * val_percent)
    n_train = len(debug_dataset) - n_val
    train_dataset, val_dataset = random_split(debug_dataset, [n_train, n_val])
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)
    print(f"Data loaded: {n_train} training samples, {n_val} validation samples.")

    # åˆå§‹åŒ–æ¨¡åž‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = MultiInputModel().to(device)
    # **é‡è¦**: å¦‚æžœæ¨¡åž‹æœ«å°¾æœ‰Sigmoidï¼Œè¯·æ³¨é‡ŠæŽ‰ã€‚æŸå¤±å‡½æ•°BCEWithLogitsLossä¼šå¤„ç†å®ƒã€‚
    # å‡è®¾æˆ‘ä»¬çš„MultiInputModelè¾“å‡ºçš„æ˜¯åŽŸå§‹logitsã€‚
    
    criterion = CombinedLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])
    
    # å­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼Œä½†æŽ¨èï¼‰
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'], eta_min=1e-6)

    # è®­ç»ƒå¾ªçŽ¯
    best_dice = -1.0
    save_dir = config['save_dir']
    os.makedirs(save_dir, exist_ok=True)

    for epoch in range(config['epochs']):
        print(f"\n--- Epoch {epoch+1}/{config['epochs']} ---")
        
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_dice = validate(model, val_loader, criterion, device)
        
        scheduler.step() # æ›´æ–°å­¦ä¹ çŽ‡

        print(f"Epoch {epoch+1} Summary: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}")

        # ä¿å­˜æ¨¡åž‹
        # ä¿å­˜æœ€æ–°çš„æ¨¡åž‹
        torch.save(model.state_dict(), os.path.join(save_dir, 'checkpoint_latest.pth'))
        
        # å¦‚æžœæ˜¯ç›®å‰æœ€å¥½çš„æ¨¡åž‹ï¼Œåˆ™å•ç‹¬ä¿å­˜
        if val_dice > best_dice:
            best_dice = val_dice
            torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pth'))
            print(f"ðŸŽ‰ New best model saved with Dice score: {best_dice:.4f}")

    print("\n--- Training Finished ---")
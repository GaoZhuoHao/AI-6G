# src/train_ddp.py (å·²ä¿®æ­£æ•°æ®é›†åˆ’åˆ†)

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import os
import numpy as np

# --- DDPç›¸å…³çš„å¯¼å…¥ ---
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

# ä»Žæˆ‘ä»¬è‡ªå·±åˆ›å»ºçš„æ¨¡å—ä¸­å¯¼å…¥
from data_loader import MultiInputSignalDataset
from model import MultiInputModel

# --- DDP è®¾ç½®ä¸Žæ¸…ç†å‡½æ•° ---
def setup_ddp(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒçŽ¯å¢ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355' # ä»»æ„æœªè¢«å ç”¨çš„ç«¯å£
    dist.init_process_group("nccl", rank=rank, world_size=world_size) # NCCLæ˜¯NVIDIA GPUæŽ¨èçš„åŽç«¯

def cleanup_ddp():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒçŽ¯å¢ƒ"""
    dist.destroy_process_group()

# --- æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ (æ— éœ€ä¿®æ”¹) ---
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth
    def forward(self, logits, targets):
        # å‡è®¾logitså·²ç»æ˜¯sigmoidä¹‹åŽçš„ç»“æžœï¼Œå¦‚æžœä¸æ˜¯ï¼Œéœ€è¦åœ¨è¿™é‡ŒåŠ ä¸Š
        probs = logits 
        intersection = (probs * targets).sum()
        dice_coeff = (2. * intersection + self.smooth) / (probs.sum() + targets.sum() + self.smooth)
        return 1 - dice_coeff

class CombinedLoss(nn.Module):
    def __init__(self, bce_weight=0.5, dice_weight=0.5):
        super(CombinedLoss, self).__init__()
        self.bce_loss = nn.BCEWithLogitsLoss()
        self.dice_loss = DiceLoss()
        self.bce_weight = bce_weight
        self.dice_weight = dice_weight
    def forward(self, logits, targets):
        bce = self.bce_loss(logits, targets)
        probs = torch.sigmoid(logits)
        dice = self.dice_loss(probs, targets)
        return self.bce_weight * bce + self.dice_weight * dice

def dice_coefficient(probs, targets, smooth=1e-6):
    intersection = (probs * targets).sum()
    return (2. * intersection + smooth) / (probs.sum() + targets.sum() + smooth)

# --- è®­ç»ƒå’ŒéªŒè¯å‡½æ•° (åŸºæœ¬ä¸å˜) ---
def train_one_epoch(model, loader, optimizer, criterion, device, rank, epoch):
    model.train()
    running_loss = 0.0
    # è®¾ç½®samplerçš„epochï¼Œä¿è¯æ¯ä¸ªepochçš„shuffleéƒ½ä¸åŒ
    loader.sampler.set_epoch(epoch)
    
    # åªåœ¨ä¸»è¿›ç¨‹(rank 0)æ˜¾ç¤ºè¿›åº¦æ¡
    progress_bar = tqdm(loader, desc=f"Training Epoch {epoch+1}", disable=(rank != 0))
    for inputs, targets in progress_bar:
        for key in inputs:
            inputs[key] = inputs[key].to(device)
        targets = targets.to(device)
        optimizer.zero_grad()
        logits = model(inputs)
        loss = criterion(logits, targets)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        if rank == 0:
            progress_bar.set_postfix(loss=f"{loss.item():.4f}")
    
    # åœ¨æ‰€æœ‰è¿›ç¨‹é—´åŒæ­¥æŸå¤±å€¼ (å¯é€‰ï¼Œä½†æœ‰åŠ©äºŽèŽ·å¾—å‡†ç¡®çš„å…¨å±€æŸå¤±)
    # æ³¨æ„ï¼šè¿™é‡Œçš„æŸå¤±æ˜¯åŸºäºŽæ¯ä¸ªè¿›ç¨‹çš„batchæ•°ï¼Œè€Œä¸æ˜¯å…¨å±€çš„
    total_loss_tensor = torch.tensor(running_loss, device=device)
    dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.SUM)
    
    # ä¸ºäº†å¾—åˆ°å…¨å‰§å¹³å‡lossï¼Œéœ€è¦é™¤ä»¥å…¨å±€çš„dataset size
    # ä½†ä¸ºäº†ç®€åŒ–ï¼Œé€šå¸¸æˆ‘ä»¬åªå…³å¿ƒä¸»è¿›ç¨‹çš„lossè¶‹åŠ¿
    # è¿™é‡Œè¿”å›žçš„æ˜¯æ‰€æœ‰è¿›ç¨‹çš„lossæ€»å’Œï¼Œæ‰“å°æ—¶å†å¤„ç†
    return total_loss_tensor.item() / len(loader.dataset) * dist.get_world_size()


def validate(model, loader, criterion, device):
    # éªŒè¯é€»è¾‘åœ¨DDPä¸­æ— éœ€ç‰¹æ®Šæ”¹åŠ¨ï¼Œå› ä¸ºæ¨¡åž‹æƒé‡æ˜¯åŒæ­¥çš„
    model.eval()
    running_loss = 0.0
    total_dice = 0.0
    rank = dist.get_rank()
    
    # ä¸ºäº†å¾—åˆ°ç²¾ç¡®çš„å…¨å±€éªŒè¯æŒ‡æ ‡ï¼Œéœ€è¦æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„ç»“æžœ
    all_losses = []
    all_dices = []
    
    with torch.no_grad():
        # æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹éªŒè¯è‡ªå·±çš„æ•°æ®å­é›†
        progress_bar = tqdm(loader, desc="Validating", disable=(rank != 0))
        for inputs, targets in progress_bar:
            for key in inputs:
                inputs[key] = inputs[key].to(device)
            targets = targets.to(device)
            
            logits = model.module(inputs) # DDPæ¨¡åž‹éœ€è¦è®¿é—®.module
            loss = criterion(logits, targets)
            
            probs = torch.sigmoid(logits)
            dice = dice_coefficient(probs, targets)
            
            all_losses.append(loss.item())
            all_dices.append(dice.item())

    # ==================== ä»£ç æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ START ====================
    # æ¯ä¸ªè¿›ç¨‹å…ˆè®¡ç®—è‡ªå·±è´Ÿè´£çš„æ•°æ®å­é›†çš„å¹³å‡losså’Œdice
    local_avg_loss = np.mean(all_losses) if all_losses else 0.0
    local_avg_dice = np.mean(all_dices) if all_dices else 0.0

    # å°†æ¯ä¸ªè¿›ç¨‹çš„å¹³å‡å€¼è½¬æ¢ä¸ºtensor
    sum_loss_tensor = torch.tensor(local_avg_loss, device=device)
    sum_dice_tensor = torch.tensor(local_avg_dice, device=device)

    # 1. ä½¿ç”¨ SUM æ“ä½œå°†æ‰€æœ‰è¿›ç¨‹çš„å¹³å‡å€¼ç´¯åŠ èµ·æ¥
    dist.all_reduce(sum_loss_tensor, op=dist.ReduceOp.SUM)
    dist.all_reduce(sum_dice_tensor, op=dist.ReduceOp.SUM)

    # 2. èŽ·å–è¿›ç¨‹æ€»æ•°
    world_size = dist.get_world_size()

    # 3. å°†æ€»å’Œé™¤ä»¥è¿›ç¨‹æ•°ï¼Œå¾—åˆ°å…¨å±€çš„å¹³å‡å€¼
    global_avg_loss = sum_loss_tensor.item() / world_size
    global_avg_dice = sum_dice_tensor.item() / world_size

    return global_avg_loss, global_avg_dice


# --- ä¸»è®­ç»ƒå‡½æ•° (å·²ä¿®æ­£) ---
def train(rank, world_size, config: dict):
    """
    DDPä¸»è®­ç»ƒæµç¨‹å‡½æ•°
    """
    setup_ddp(rank, world_size)
    
    # æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨è‡ªå·±çš„GPU
    device = rank
    torch.cuda.set_device(device)
    
    # ==================== ä»£ç æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ START ====================
    # 1. å®žä¾‹åŒ–å®Œæ•´æ•°æ®é›†
    full_dataset = MultiInputSignalDataset(h5_path=config['data_path'], config=config)

    # 2. å®šä¹‰åˆ’åˆ†å‚æ•°
    validation_split = config.get('validation_split', 0.2)
    random_seed = config.get('seed', 42)
    
    # 3. è®¡ç®—åˆ’åˆ†å¤§å°
    total_size = len(full_dataset)
    val_size = int(validation_split * total_size)
    train_size = total_size - val_size
    
    # 4. ä½¿ç”¨å›ºå®šçš„éšæœºç§å­è¿›è¡Œæ•°æ®é›†åˆ’åˆ†ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹åˆ’åˆ†ç»“æžœä¸€è‡´
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(random_seed)
    )
    
    if rank == 0:
        print(f"Dataset successfully split. Train size: {len(train_dataset)}, Val size: {len(val_dataset)}")

    # 5. ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«åˆ›å»ºç‹¬ç«‹çš„åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)
    
    # 6. ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«åˆ›å»ºç‹¬ç«‹çš„DataLoader
    # batch_sizeçŽ°åœ¨æ˜¯æ¯ä¸ªGPUçš„batch_size
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True,
        shuffle=False # shuffleå¿…é¡»ä¸ºFalseï¼Œå› ä¸ºsamplerå·²ç»å¤„ç†äº†
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        sampler=val_sampler,
        num_workers=4,
        pin_memory=True,
        shuffle=False
    )
    # ==================== ä»£ç æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ END ====================

    # æ¨¡åž‹åˆå§‹åŒ–å¹¶ç§»åŠ¨åˆ°å¯¹åº”GPUï¼Œç„¶åŽç”¨DDPåŒ…è£…
    model = MultiInputModel().to(device)
    model = DDP(model, device_ids=[rank])
    
    criterion = CombinedLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])
    
    best_dice = -1.0
    
    for epoch in range(config['epochs']):
        if rank == 0:
            print(f"\n--- Epoch {epoch+1}/{config['epochs']} ---")
        
        # è®­ç»ƒæ—¶ï¼Œä½¿ç”¨train_loader
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, rank, epoch)
        
        # éªŒè¯æ—¶ï¼Œä½¿ç”¨val_loader
        val_loss, val_dice = validate(model, val_loader, criterion, device)

        # åªåœ¨ä¸»è¿›ç¨‹(rank 0)è¿›è¡Œæ—¥å¿—æ‰“å°å’Œæ¨¡åž‹ä¿å­˜
        if rank == 0:
            print(f"Epoch {epoch+1} Summary: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}")
            
            save_dir = config['save_dir']
            os.makedirs(save_dir, exist_ok=True)
            
            torch.save(model.module.state_dict(), os.path.join(save_dir, 'checkpoint_latest.pth'))
            
            if val_dice > best_dice:
                best_dice = val_dice
                torch.save(model.module.state_dict(), os.path.join(save_dir, 'best_model.pth'))
                print(f"ðŸŽ‰ New best model saved with Dice score: {best_dice:.4f}")

    cleanup_ddp()